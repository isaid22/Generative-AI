## Transcript-Based QC Metrics

Below are some of metrics related to transcript of conversation:

| **Metric**                                 | **How to Compute or Estimate**                                                                                                                                                   |
| ------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Script Adherence Score (%)**          | Use keyword or pattern matching (e.g., did the transcript include "Hello, thank you for calling"?) and check for required components. Build a checklist and count matched items. |
| **2. Correct Intent Classification (Y/N)** | Use a classifier (e.g., fine-tuned BERT or LLM) to label the intent and compare against human-annotated ground truth.                                                            |
| **3. Hallucination Count**                 | Run fact-checking or NLI (Natural Language Inference) tools; compare LLM output vs original transcript content. Flag generated statements that don‚Äôt match the source.           |
| **4. Question Resolution Rate (%)**        | Identify whether a customer‚Äôs question was followed by a direct, complete answer. Use a QA model or semantic entailment tools to judge answer completeness.                      |
| **5. Disposition Match Flag**              | Use text classification to assign a disposition from the transcript, then compare it to the expected CRM/system disposition (e.g., sale, no-sale, callback).                     |
| **6. PII/PCI Leakage Flags**               | Use regex + NER models (e.g., `spaCy`, AWS Comprehend) to detect SSN, DOB, credit card numbers, etc.                                                                             |
| **7. Customer Sentiment Classification**   | Use a sentiment analysis model (e.g., VADER, BERT-based classifiers) on customer utterances only.                                                                                |
| **8. Opt-Out Detection (Y/N)**             | Pattern match for phrases like ‚Äúdo not call‚Äù, ‚Äúunsubscribe‚Äù, or use intent classification for opt-out detection.                                                                 |
| **9. Whisper Notes Accuracy (%)**          | Compare extracted metadata ("whisper notes") with transcript content using fuzzy string match or embedding similarity (e.g., cosine similarity of sentence embeddings).          |
| **10. Response Lag Estimate (ms)**         | Requires timestamped transcript. Calculate lag between customer‚Äôs last word and agent‚Äôs first word. Normalize per exchange and average.                                          |


### Implementation sugestions

#### Tools & Libraries:
Text Classification: scikit-learn, Hugging Face Transformers

NER/PII Detection: spaCy, Presidio, AWS Comprehend, regex

Sentiment Analysis: VADER, TextBlob, fine-tuned distilBERT

Hallucination Detection: No turnkey solution; use LLM + reference checking or entailment (e.g., DeBERTa-based NLI)

Keyword Matching: Python‚Äôs re, or flashtext for efficient keyword search

Timestamp Handling: If timestamps are included in JSON or CSV, use pandas to compute time gaps

For Whisper Note Accureacy, following are some realistic means of calculation:

üîπ Keyword or Phrase Overlap
Method: Use TF-IDF or bag-of-words models to detect if the same key concepts appear in both.

Useful for: Call center contexts (e.g., ‚Äúclose account‚Äù, ‚Äúcredit card limit‚Äù).

Tool: sklearn.feature_extraction.text.TfidfVectorizer

üîπ Semantic Similarity (Embeddings)
Method: Convert both the whisper note and transcript excerpt into embeddings using:

Sentence-BERT

OpenAI embeddings

Universal Sentence Encoder

Compute cosine similarity between them.

Pros: Captures true paraphrasing.

Threshold: Typically 0.75‚Äì0.85.

Example Tools: sentence-transformers, openai, tensorflow-hub